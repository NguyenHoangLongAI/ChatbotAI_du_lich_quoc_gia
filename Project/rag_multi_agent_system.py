"""
RAG Multi-Agent System with LangGraph for B√£i Ch√°y Tourism
UPDATED: T√≠ch h·ª£p ƒë·∫ßy ƒë·ªß image_url v√† url b√†i vi·∫øt v√†o c√¢u tr·∫£ l·ªùi
UPDATED v2: Th√™m Real LLM Streaming support

H·ªá th·ªëng ƒëa t√°c nh√¢n v·ªõi workflow th√¥ng minh x·ª≠ l√Ω:
1. T∆∞ v·∫•n d·ªãch v·ª• du l·ªãch (c√≥ h√¨nh ·∫£nh v√† link b√†i vi·∫øt)
2. Gi·∫£i ƒë√°p quy ƒë·ªãnh & t√†i li·ªáu
3. H∆∞·ªõng d·∫´n ƒë·∫∑t h√†ng & ho√†n th√†nh booking
"""

from typing import TypedDict, Annotated, List, Dict, Optional, Generator, AsyncGenerator
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import tool
import operator
from datetime import datetime
import json
import logging
import os

# Import your existing DAOs
import sys
sys.path.append('/mnt/user-data/uploads')
from crawl_baichay_service.tourism_dao import BaiChayTourismDAO
from document_db.tourism_document_dao import TourismDocumentDAO
from baichay_db.customer_dao import CustomerDAO
from document_api.embedding_service import EmbeddingService
from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage
from langchain_core.output_parsers import StrOutputParser

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================================
# OPENAI LLM WRAPPER
# ============================================================================

class OpenAILLMWrapper:
    """Wrapper for OpenAI Chat models (GPT-4o)"""

    def __init__(
        self,
        model: str = "gpt-4o",
        temperature: float = 0.1,
        streaming: bool = False
    ):
        self.model = model
        self.temperature = temperature
        self.streaming = streaming

        logger.info(f"ü§ñ Initializing OpenAI LLM: {model}")

        # Get API key from environment
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")

        self.llm = ChatOpenAI(
            model=model,
            temperature=temperature,
            streaming=streaming,
            api_key=api_key
        )

        self.output_parser = StrOutputParser()
        logger.info(f"‚úÖ OpenAI {model} initialized successfully")

    def invoke(self, messages: list, **kwargs) -> AIMessage:
        """Non-streaming invoke"""
        try:
            response = self.llm.invoke(messages, **kwargs)
            return AIMessage(content=response.content)

        except Exception as e:
            logger.error(f"‚ùå OpenAI invoke error: {e}", exc_info=True)
            return AIMessage(content=f"L·ªói x·ª≠ l√Ω OpenAI: {str(e)}")

    def stream(self, messages: list, **kwargs):
        """Streaming (sync) - yields real chunks t·ª´ LLM"""
        try:
            for chunk in self.llm.stream(messages, **kwargs):
                if chunk.content:
                    yield chunk.content
        except Exception as e:
            logger.error(f"‚ùå OpenAI streaming error: {e}", exc_info=True)
            yield f"\n\n[L·ªói streaming: {str(e)}]"

    async def astream(self, messages: list, **kwargs):
        """Async streaming - yields real chunks t·ª´ LLM"""
        try:
            async for chunk in self.llm.astream(messages, **kwargs):
                if chunk.content:
                    yield chunk.content
        except Exception as e:
            logger.error(f"‚ùå OpenAI async streaming error: {e}", exc_info=True)
            yield f"\n\n[L·ªói streaming: {str(e)}]"

    def bind_tools(self, tools: list):
        """Bind tools to LLM"""
        self.llm = self.llm.bind_tools(tools)
        return self


# ============================================================================
# STATE DEFINITION
# ============================================================================

class AgentState(TypedDict):
    """Tr·∫°ng th√°i c·ªßa h·ªá th·ªëng multi-agent"""
    messages: Annotated[List, operator.add]  # L·ªãch s·ª≠ h·ªôi tho·∫°i
    user_query: str                           # Query g·ªëc t·ª´ user
    query_type: str                           # tourism | document | booking | unknown
    search_results: Optional[Dict]            # K·∫øt qu·∫£ t√¨m ki·∫øm t·ª´ vector DB
    selected_services: List[Dict]             # D·ªãch v·ª• kh√°ch h√†ng ƒë√£ ch·ªçn
    booking_info: Optional[Dict]              # Th√¥ng tin ƒë·∫∑t h√†ng
    customer_info: Optional[Dict]             # Th√¥ng tin kh√°ch h√†ng
    next_action: str                          # Action ti·∫øp theo
    final_response: str                       # Response cu·ªëi c√πng
    # ‚≠ê Th√™m field ƒë·ªÉ l∆∞u messages c·∫ßn streaming
    stream_messages: Optional[List]           # Messages cho LLM streaming (cu·ªëi workflow)
    stream_system_prompt: Optional[str]       # System prompt cho streaming


# ============================================================================
# TOOLS - K·∫øt n·ªëi v·ªõi Database
# ============================================================================

class RAGTools:
    """Tools ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi Milvus collections"""

    def __init__(self, milvus_host="localhost", milvus_port="19530"):
        self.tourism_dao = BaiChayTourismDAO(host=milvus_host, port=milvus_port)
        self.document_dao = TourismDocumentDAO(host=milvus_host, port=milvus_port)
        self.customer_dao = CustomerDAO(host=milvus_host, port=milvus_port)
        self.embedding_service = EmbeddingService()

        logger.info("‚úÖ RAG Tools initialized")

    def search_tourism_services(self, query: str, top_k: int = 5) -> str:
        """
        T√¨m ki·∫øm d·ªãch v·ª• du l·ªãch (tour, ƒëi·ªÉm ƒë·∫øn, kh√°ch s·∫°n, nh√† h√†ng...)
        UPDATED: Bao g·ªìm image_url v√† url b√†i vi·∫øt
        """
        try:
            query_vector = self.embedding_service.get_embedding(query)
            results = self.tourism_dao.search_by_description(
                query_vector=query_vector,
                top_k=top_k
            )

            formatted_results = []
            for result in results:
                formatted_results.append({
                    "id": result.get("id"),
                    "name": result.get("name"),
                    "type": result.get("type"),
                    "sub_type": result.get("sub_type"),
                    "location": result.get("location"),
                    "address": result.get("address"),
                    "description": result.get("description", "")[:500],
                    "price_range": result.get("price_range"),
                    "price_min": result.get("price_min"),
                    "price_max": result.get("price_max"),
                    "rating": result.get("rating"),
                    "opening_hours": result.get("opening_hours"),
                    "image_url": result.get("image_url", ""),
                    "url": result.get("url", ""),
                    "similarity_score": round(result.get("score", 0), 3)
                })

            logger.info(f"‚úÖ Found {len(formatted_results)} services")
            return json.dumps(formatted_results, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"‚ùå Search tourism error: {e}")
            return json.dumps({"error": str(e)})

    @tool
    def search_documents(self, query: str, top_k: int = 3) -> str:
        """T√¨m ki·∫øm t√†i li·ªáu quy ƒë·ªãnh, h∆∞·ªõng d·∫´n du l·ªãch"""
        try:
            query_vector = self.embedding_service.get_embedding(query)
            search_params = {
                "metric_type": "COSINE",
                "params": {"ef": 100}
            }
            results = self.document_dao.doc_collection.search(
                data=[query_vector],
                anns_field="description_vector",
                param=search_params,
                limit=top_k,
                output_fields=["document_id", "description"]
            )
            formatted_results = []
            for hits in results:
                for hit in hits:
                    formatted_results.append({
                        "document_id": hit.entity.get("document_id"),
                        "content": hit.entity.get("description"),
                        "similarity_score": hit.score
                    })
            return json.dumps(formatted_results, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"‚ùå Search document error: {e}")
            return json.dumps({"error": str(e)})

    @tool
    def get_service_by_id(self, service_id: int) -> str:
        """L·∫•y th√¥ng tin chi ti·∫øt d·ªãch v·ª• theo ID"""
        try:
            result = self.tourism_dao.get_by_id(service_id)
            if result:
                return json.dumps(result, ensure_ascii=False, indent=2)
            else:
                return json.dumps({"error": "Service not found"})
        except Exception as e:
            return json.dumps({"error": str(e)})

    @tool
    def create_customer_booking(
        self,
        name: str,
        phone: str,
        service_ids: List[int],
        service_descriptions: str,
        checkin_date: str,
        checkout_date: str
    ) -> str:
        """T·∫°o booking cho kh√°ch h√†ng"""
        try:
            checkin = datetime.strptime(checkin_date, "%Y-%m-%d")
            checkout = datetime.strptime(checkout_date, "%Y-%m-%d")
            description = f"ƒê·∫∑t d·ªãch v·ª• du l·ªãch B√£i Ch√°y. D·ªãch v·ª•: {service_descriptions}. IDs: {service_ids}"
            description_vector = self.embedding_service.get_embedding(description)
            customer_data = {
                "name": name,
                "phone": phone,
                "checkin_time": checkin,
                "checkout_time": checkout,
                "description": description,
                "description_vector": description_vector
            }
            customer_id = self.customer_dao.insert_customer(customer_data)
            result = {
                "status": "success",
                "customer_id": customer_id,
                "name": name,
                "phone": phone,
                "checkin": checkin_date,
                "checkout": checkout_date,
                "services": service_ids,
                "message": "Booking ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!"
            }
            return json.dumps(result, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"‚ùå Create booking error: {e}")
            return json.dumps({"status": "error", "message": str(e)})


# ============================================================================
# SYSTEM PROMPTS (t√°ch ra ƒë·ªÉ t√°i d√πng cho c·∫£ invoke v√† stream)
# ============================================================================

TOURISM_SYSTEM_PROMPT = """B·∫°n l√† chuy√™n gia t∆∞ v·∫•n du l·ªãch B√£i Ch√°y - Qu·∫£ng Ninh.

NHI·ªÜM V·ª§:
D·ª±a v√†o k·∫øt qu·∫£ t√¨m ki·∫øm, t∆∞ v·∫•n cho kh√°ch h√†ng v·ªÅ c√°c d·ªãch v·ª• du l·ªãch.

FORMAT TR·∫¢ L·ªúI B·∫ÆT BU·ªòC:
V·ªõi m·ªói d·ªãch v·ª•, tr√¨nh b√†y theo c·∫•u tr√∫c sau:

---
### üè® [T√™n d·ªãch v·ª•] {rating > 0 ? '‚≠ê [rating]/5' : ''}

**üìç ƒê·ªãa ch·ªâ:** [address ho·∫∑c location]

**üí∞ Gi√°:** [price_range]

**üìù M√¥ t·∫£:** [T√≥m t·∫Øt description, kho·∫£ng 4-5 c√¢u]

**üñºÔ∏è H√¨nh ·∫£nh:**
{image_url c√≥ gi√° tr·ªã ? hi·ªÉn th·ªã URL : "Ch∆∞a c√≥ h√¨nh ·∫£nh"}

**üîó Xem chi ti·∫øt:** {url c√≥ gi√° tr·ªã ? hi·ªÉn th·ªã URL : "Li√™n h·ªá ƒë·ªÉ bi·∫øt th√™m"}

**üÜî ID ƒë·ªÉ ƒë·∫∑t:** [id]

---

NGUY√äN T·∫ÆC QUAN TR·ªåNG:
1. ‚úÖ LU√îN LU√îN hi·ªÉn th·ªã image_url n·∫øu c√≥ (kh√¥ng ƒë∆∞·ª£c b·ªè qua)
2. ‚úÖ LU√îN LU√îN hi·ªÉn th·ªã url b√†i vi·∫øt n·∫øu c√≥
3. ‚úÖ Format URL r√µ r√†ng, d·ªÖ click (markdown link ho·∫∑c plain URL)
4. ‚úÖ S·∫Øp x·∫øp theo similarity_score (cao nh·∫•t tr∆∞·ªõc)
5. ‚úÖ K·∫øt th√∫c b·∫±ng c√¢u h·ªèi: "B·∫°n c√≥ mu·ªën ƒë·∫∑t d·ªãch v·ª• n√†o kh√¥ng? H√£y cho t√¥i bi·∫øt ID d·ªãch v·ª•, t√¥i s·∫Ω ti·∫øn h√†nh booking cho b·∫°n."

PHONG C√ÅCH: Th√¢n thi·ªán, nhi·ªát t√¨nh, chuy√™n nghi·ªáp.

KH√îNG ƒê∆Ø·ª¢C:
- ‚ùå B·ªè qua image_url ho·∫∑c url n·∫øu c√≥
- ‚ùå G·ªôp chung nhi·ªÅu d·ªãch v·ª• v√†o m·ªôt m·ª•c"""

DOCUMENT_SYSTEM_PROMPT = """B·∫°n l√† chuy√™n gia t∆∞ v·∫•n quy ƒë·ªãnh du l·ªãch B√£i Ch√°y.

NHI·ªÜM V·ª§:
1. ƒê·ªçc k·ªπ n·ªôi dung t√†i li·ªáu t√¨m ƒë∆∞·ª£c
2. Tr·∫£ l·ªùi ch√≠nh x√°c d·ª±a tr√™n t√†i li·ªáu
3. Tr√≠ch d·∫´n ngu·ªìn (document_id) n·∫øu c√≥

NGUY√äN T·∫ÆC:
- Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu t√¨m ƒë∆∞·ª£c
- N·∫øu kh√¥ng t√¨m th·∫•y: "T√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin n√†y trong t√†i li·ªáu"
- Tr√¨nh b√†y r√µ r√†ng, d·ªÖ hi·ªÉu
- G·ª£i √Ω li√™n h·ªá hotline n·∫øu c·∫ßn

PHONG C√ÅCH: Chuy√™n nghi·ªáp, ch√≠nh x√°c, h·ªØu √≠ch."""

BOOKING_SYSTEM_PROMPT = """B·∫°n l√† chuy√™n vi√™n ƒë·∫∑t tour du l·ªãch B√£i Ch√°y.

NHI·ªÜM V·ª§:
1. Thu th·∫≠p ƒë·∫ßy ƒë·ªß th√¥ng tin:
   - H·ªç t√™n kh√°ch h√†ng
   - S·ªë ƒëi·ªán tho·∫°i
   - ID d·ªãch v·ª• ƒë√£ ch·ªçn (n·∫øu c√≥ t·ª´ h·ªôi tho·∫°i tr∆∞·ªõc)
   - Ng√†y check-in (YYYY-MM-DD)
   - Ng√†y check-out (YYYY-MM-DD)

2. Ph√¢n t√≠ch xem ƒë√£ ƒë·ªß th√¥ng tin ch∆∞a:
   - N·∫øu ƒê·ª¶: Tr·∫£ v·ªÅ JSON v·ªõi format:
     {"action": "create_booking", "name": "...", "phone": "...", "service_ids": [...], "checkin": "YYYY-MM-DD", "checkout": "YYYY-MM-DD", "description": "..."}
   
   - N·∫øu THI·∫æU: H·ªèi th√™m th√¥ng tin c√≤n thi·∫øu

PHONG C√ÅCH: Chuy√™n nghi·ªáp, th√¢n thi·ªán, x√°c nh·∫≠n l·∫°i th√¥ng tin tr∆∞·ªõc khi ƒë·∫∑t."""


# ============================================================================
# AGENT NODES
# ============================================================================

class TourismAgents:
    """C√°c agent chuy√™n bi·ªát trong h·ªá th·ªëng"""

    def __init__(
            self,
            tools: RAGTools,
            openai_model: str = "gpt-4o"
    ):
        self.tools = tools

        self.llm = OpenAILLMWrapper(
            model=openai_model,
            temperature=0.1
        )

        self.tool_list = [
            tools.search_tourism_services,
            tools.search_documents,
            tools.get_service_by_id,
            tools.create_customer_booking
        ]

    def router_agent(self, state: AgentState) -> AgentState:
        """Agent ph√¢n lo·∫°i: X√°c ƒë·ªãnh lo·∫°i query"""
        logger.info("üîÄ Router Agent analyzing query...")

        user_query = state["user_query"]

        system_prompt = """B·∫°n l√† tr·ª£ l√Ω ph√¢n lo·∫°i c√¢u h·ªèi du l·ªãch B√£i Ch√°y.

Ph√¢n lo·∫°i c√¢u h·ªèi th√†nh 1 trong 3 lo·∫°i:
- "tourism": T√¨m tour, ƒëi·ªÉm ƒë·∫øn, kh√°ch s·∫°n, nh√† h√†ng, gi√° c·∫£
- "document": H·ªèi v·ªÅ quy ƒë·ªãnh, khi·∫øu n·∫°i, th·ªß t·ª•c, ch√≠nh s√°ch
- "booking": Kh√°ch mu·ªën ƒë·∫∑t d·ªãch v·ª•, cung c·∫•p th√¥ng tin c√° nh√¢n

CH·ªà TR·∫¢ V·ªÄ 1 T·ª™: tourism, document, ho·∫∑c booking
KH√îNG GI·∫¢I TH√çCH, CH·ªà TR·∫¢ V·ªÄ T·ª™ KH√ìA."""

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"Ph√¢n lo·∫°i c√¢u h·ªèi sau:\n{user_query}")
        ]

        response = self.llm.invoke(messages)
        query_type = response.content.strip().lower()

        if "tourism" in query_type:
            query_type = "tourism"
        elif "document" in query_type:
            query_type = "document"
        elif "booking" in query_type:
            query_type = "booking"
        else:
            query_type = "tourism"

        logger.info(f"‚úÖ Query type: {query_type}")

        state["query_type"] = query_type
        state["messages"].append(AIMessage(content=f"[Query classified as: {query_type}]"))

        return state

    def tourism_advisor_agent(self, state: AgentState) -> AgentState:
        """
        Agent t∆∞ v·∫•n d·ªãch v·ª• du l·ªãch
        ‚≠ê Khi streaming=False: d√πng invoke nh∆∞ c≈©
        ‚≠ê Khi streaming=True: l∆∞u messages v√†o state, KH√îNG invoke LLM ·ªü ƒë√¢y
        """
        logger.info("üèñÔ∏è Tourism Advisor Agent working...")

        user_query = state["user_query"]

        # B∆∞·ªõc 1: Search (lu√¥n ch·∫°y, kh√¥ng ph·ª• thu·ªôc streaming)
        logger.info("üìû Calling search_tourism_services tool...")
        search_results = self.tools.search_tourism_services(query=user_query, top_k=5)

        # L∆∞u search results
        try:
            state["search_results"] = json.loads(search_results)
        except:
            state["search_results"] = {}

        # B∆∞·ªõc 2: Chu·∫©n b·ªã messages cho LLM
        llm_messages = [
            SystemMessage(content=TOURISM_SYSTEM_PROMPT),
            HumanMessage(content=f"C√¢u h·ªèi: {user_query}\n\nK·∫øt qu·∫£ t√¨m ki·∫øm:\n{search_results}\n\nH√£y t∆∞ v·∫•n cho kh√°ch h√†ng.")
        ]

        # ‚≠ê L∆∞u messages v√†o state ƒë·ªÉ streaming endpoint d√πng
        state["stream_messages"] = llm_messages
        state["stream_system_prompt"] = TOURISM_SYSTEM_PROMPT

        # B∆∞·ªõc 3: Invoke non-streaming (d√πng cho /chat endpoint b√¨nh th∆∞·ªùng)
        response = self.llm.invoke(llm_messages)
        state["messages"].append(response)
        state["final_response"] = response.content
        state["next_action"] = "end"

        logger.info("‚úÖ Tourism advice generated")
        return state

    def document_advisor_agent(self, state: AgentState) -> AgentState:
        """Agent gi·∫£i ƒë√°p quy ƒë·ªãnh & t√†i li·ªáu"""
        logger.info("üìö Document Advisor Agent working...")

        user_query = state["user_query"]

        # B∆∞·ªõc 1: Search
        logger.info("üìû Calling search_documents tool...")
        search_results = self.tools.search_documents.invoke({"query": user_query, "top_k": 3})

        try:
            state["search_results"] = json.loads(search_results)
        except:
            state["search_results"] = {}

        # B∆∞·ªõc 2: Chu·∫©n b·ªã messages
        llm_messages = [
            SystemMessage(content=DOCUMENT_SYSTEM_PROMPT),
            HumanMessage(content=f"C√¢u h·ªèi: {user_query}\n\nT√†i li·ªáu t√¨m ƒë∆∞·ª£c:\n{search_results}\n\nH√£y tr·∫£ l·ªùi c√¢u h·ªèi.")
        ]

        state["stream_messages"] = llm_messages
        state["stream_system_prompt"] = DOCUMENT_SYSTEM_PROMPT

        # B∆∞·ªõc 3: Invoke non-streaming
        response = self.llm.invoke(llm_messages)
        state["messages"].append(response)
        state["search_results"] = json.loads(search_results) if isinstance(search_results, str) else search_results
        state["final_response"] = response.content
        state["next_action"] = "end"

        return state

    def booking_agent(self, state: AgentState) -> AgentState:
        """Agent x·ª≠ l√Ω ƒë·∫∑t h√†ng"""
        logger.info("üé´ Booking Agent working...")

        user_query = state["user_query"]
        messages_history = state["messages"]

        conversation_text = "\n".join([
            f"{msg.__class__.__name__}: {msg.content}"
            for msg in messages_history[-3:] if hasattr(msg, 'content')
        ])

        llm_messages = [
            SystemMessage(content=BOOKING_SYSTEM_PROMPT),
            HumanMessage(content=f"L·ªãch s·ª≠ h·ªôi tho·∫°i:\n{conversation_text}\n\nTin nh·∫Øn m·ªõi: {user_query}\n\nPh√¢n t√≠ch v√† x·ª≠ l√Ω.")
        ]

        state["stream_messages"] = llm_messages
        state["stream_system_prompt"] = BOOKING_SYSTEM_PROMPT

        response = self.llm.invoke(llm_messages)
        response_text = response.content

        # Check if response contains booking action
        if '"action": "create_booking"' in response_text or "'action': 'create_booking'" in response_text:
            try:
                import re
                json_match = re.search(r'\{[^}]+\}', response_text, re.DOTALL)
                if json_match:
                    booking_data = json.loads(json_match.group())
                    logger.info("üìû Creating customer booking...")
                    result = self.tools.create_customer_booking.invoke(booking_data)
                    state["messages"].append(AIMessage(content=f"Booking result: {result}"))
                    state["final_response"] = f"‚úÖ ƒê·∫∑t h√†ng th√†nh c√¥ng!\n\n{result}"
                    state["booking_info"] = json.loads(result)
                else:
                    state["final_response"] = response_text
            except Exception as e:
                logger.error(f"Booking error: {e}")
                state["final_response"] = f"Xin l·ªói, c√≥ l·ªói khi t·∫°o booking: {e}\n\nVui l√≤ng th·ª≠ l·∫°i."
        else:
            state["final_response"] = response_text

        state["messages"].append(response)
        state["next_action"] = "end"

        return state


# ============================================================================
# WORKFLOW BUILDER
# ============================================================================

def build_rag_workflow(openai_model: str = "gpt-4o") -> StateGraph:
    """Build the multi-agent RAG workflow"""

    tools = RAGTools()
    agents = TourismAgents(tools, openai_model=openai_model)

    workflow = StateGraph(AgentState)

    workflow.add_node("router", agents.router_agent)
    workflow.add_node("tourism_advisor", agents.tourism_advisor_agent)
    workflow.add_node("document_advisor", agents.document_advisor_agent)
    workflow.add_node("booking_agent", agents.booking_agent)

    workflow.set_entry_point("router")

    def route_query(state: AgentState) -> str:
        query_type = state.get("query_type", "tourism")
        if query_type == "document":
            return "document_advisor"
        elif query_type == "booking":
            return "booking_agent"
        else:
            return "tourism_advisor"

    workflow.add_conditional_edges(
        "router",
        route_query,
        {
            "tourism_advisor": "tourism_advisor",
            "document_advisor": "document_advisor",
            "booking_agent": "booking_agent"
        }
    )

    workflow.add_edge("tourism_advisor", END)
    workflow.add_edge("document_advisor", END)
    workflow.add_edge("booking_agent", END)

    return workflow.compile()


# ============================================================================
# STREAMING WORKFLOW BUILDER
# ‚≠ê Workflow n√†y: router + search ch·∫°y b√¨nh th∆∞·ªùng, KH√îNG invoke LLM cu·ªëi
# ============================================================================

def build_rag_workflow_for_streaming(openai_model: str = "gpt-4o"):
    """
    Build workflow ph·ª•c v·ª• streaming.
    Workflow ch·∫°y ƒë·∫øn khi search xong, l∆∞u messages v√†o state.
    Sau ƒë√≥ caller s·∫Ω stream LLM b√™n ngo√†i workflow.
    """
    tools = RAGTools()

    # T·∫°o agents nh∆∞ng override ƒë·ªÉ KH√îNG invoke LLM cu·ªëi trong advisor nodes
    class StreamingAgents(TourismAgents):

        def tourism_advisor_agent(self, state: AgentState) -> AgentState:
            """Tourism advisor: ch·ªâ search, KH√îNG invoke LLM"""
            logger.info("üèñÔ∏è [STREAM] Tourism Advisor: searching only...")
            user_query = state["user_query"]

            search_results = self.tools.search_tourism_services(query=user_query, top_k=5)

            try:
                state["search_results"] = json.loads(search_results)
            except:
                state["search_results"] = {}

            llm_messages = [
                SystemMessage(content=TOURISM_SYSTEM_PROMPT),
                HumanMessage(content=f"C√¢u h·ªèi: {user_query}\n\nK·∫øt qu·∫£ t√¨m ki·∫øm:\n{search_results}\n\nH√£y t∆∞ v·∫•n cho kh√°ch h√†ng.")
            ]

            state["stream_messages"] = llm_messages
            state["stream_system_prompt"] = TOURISM_SYSTEM_PROMPT
            state["final_response"] = ""  # S·∫Ω ƒë∆∞·ª£c fill b·ªüi streaming
            state["next_action"] = "stream"

            logger.info(f"‚úÖ [STREAM] Search done, {len(state['search_results'])} results ready for streaming")
            return state

        def document_advisor_agent(self, state: AgentState) -> AgentState:
            """Document advisor: ch·ªâ search, KH√îNG invoke LLM"""
            logger.info("üìö [STREAM] Document Advisor: searching only...")
            user_query = state["user_query"]

            search_results = self.tools.search_documents.invoke({"query": user_query, "top_k": 3})

            try:
                state["search_results"] = json.loads(search_results)
            except:
                state["search_results"] = {}

            llm_messages = [
                SystemMessage(content=DOCUMENT_SYSTEM_PROMPT),
                HumanMessage(content=f"C√¢u h·ªèi: {user_query}\n\nT√†i li·ªáu t√¨m ƒë∆∞·ª£c:\n{search_results}\n\nH√£y tr·∫£ l·ªùi c√¢u h·ªèi.")
            ]

            state["stream_messages"] = llm_messages
            state["stream_system_prompt"] = DOCUMENT_SYSTEM_PROMPT
            state["final_response"] = ""
            state["next_action"] = "stream"

            return state

        def booking_agent(self, state: AgentState) -> AgentState:
            """Booking agent: ch·ªâ chu·∫©n b·ªã context, KH√îNG invoke LLM (tr·ª´ khi c·∫ßn t·∫°o booking)"""
            logger.info("üé´ [STREAM] Booking Agent working...")
            user_query = state["user_query"]
            messages_history = state["messages"]

            conversation_text = "\n".join([
                f"{msg.__class__.__name__}: {msg.content}"
                for msg in messages_history[-3:] if hasattr(msg, 'content')
            ])

            llm_messages = [
                SystemMessage(content=BOOKING_SYSTEM_PROMPT),
                HumanMessage(content=f"L·ªãch s·ª≠ h·ªôi tho·∫°i:\n{conversation_text}\n\nTin nh·∫Øn m·ªõi: {user_query}\n\nPh√¢n t√≠ch v√† x·ª≠ l√Ω.")
            ]

            state["stream_messages"] = llm_messages
            state["stream_system_prompt"] = BOOKING_SYSTEM_PROMPT
            state["final_response"] = ""
            state["next_action"] = "stream"

            return state

    agents = StreamingAgents(tools, openai_model=openai_model)

    workflow = StateGraph(AgentState)
    workflow.add_node("router", agents.router_agent)
    workflow.add_node("tourism_advisor", agents.tourism_advisor_agent)
    workflow.add_node("document_advisor", agents.document_advisor_agent)
    workflow.add_node("booking_agent", agents.booking_agent)

    workflow.set_entry_point("router")

    def route_query(state: AgentState) -> str:
        query_type = state.get("query_type", "tourism")
        if query_type == "document":
            return "document_advisor"
        elif query_type == "booking":
            return "booking_agent"
        else:
            return "tourism_advisor"

    workflow.add_conditional_edges(
        "router",
        route_query,
        {
            "tourism_advisor": "tourism_advisor",
            "document_advisor": "document_advisor",
            "booking_agent": "booking_agent"
        }
    )

    workflow.add_edge("tourism_advisor", END)
    workflow.add_edge("document_advisor", END)
    workflow.add_edge("booking_agent", END)

    return workflow.compile(), tools


# ============================================================================
# MAIN INTERFACE
# ============================================================================

class BaiChayRAGSystem:
    """Main interface for the RAG system with OpenAI GPT-4o"""

    def __init__(self, openai_model: str = "gpt-4o"):
        logger.info("üöÄ Initializing B√£i Ch√°y RAG System with OpenAI GPT-4o...")
        logger.info(f"   Model: {openai_model}")
        logger.info("   ‚úÖ Streaming support enabled")

        self.openai_model = openai_model

        # Workflow cho non-streaming (invoke nh∆∞ c≈©)
        self.workflow = build_rag_workflow(openai_model=openai_model)

        # Workflow + LLM cho streaming
        self.streaming_workflow, self._tools = build_rag_workflow_for_streaming(openai_model=openai_model)

        # LLM ri√™ng ƒë·ªÉ stream
        api_key = os.getenv("OPENAI_API_KEY")
        self._streaming_llm = ChatOpenAI(
            model=openai_model,
            temperature=0.1,
            streaming=True,
            api_key=api_key
        )

        logger.info("‚úÖ RAG System ready! (non-stream + stream)")

    def question(self, question: str, history: List = None) -> Dict:
        """Alias method cho /chat endpoint"""
        return self.process_query(user_query=question, conversation_history=history)

    def process_query(self, user_query: str, conversation_history: List = None) -> Dict:
        """
        Non-streaming: ch·∫°y workflow ƒë·∫ßy ƒë·ªß, tr·∫£ v·ªÅ response string.
        Gi·ªØ nguy√™n behavior c≈©.
        """
        initial_state = {
            "messages": conversation_history or [],
            "user_query": user_query,
            "query_type": "unknown",
            "search_results": None,
            "selected_services": [],
            "booking_info": None,
            "customer_info": None,
            "next_action": "",
            "final_response": "",
            "stream_messages": None,
            "stream_system_prompt": None,
        }

        logger.info(f"üîÑ [NON-STREAM] Processing query: {user_query}")
        final_state = self.workflow.invoke(initial_state)

        return {
            "response": final_state.get("final_response", "Xin l·ªói, t√¥i ch∆∞a hi·ªÉu c√¢u h·ªèi."),
            "query_type": final_state.get("query_type"),
            "messages": final_state.get("messages", [])
        }

    def stream_query(self, user_query: str, conversation_history: List = None) -> Generator[str, None, None]:
        """
        ‚≠ê SYNC STREAMING: Ch·∫°y workflow ƒë·ªÉ search, sau ƒë√≥ stream LLM response real-time.

        Yield t·ª´ng JSON string theo format:
            {"type": "start",  "content": null, "references": null, "status": "processing"}
            {"type": "chunk",  "content": "text...", "references": null, "status": null}
            {"type": "end",    "content": null, "references": null, "status": "done"}
            {"type": "error",  "content": "msg", "references": null, "status": "error"}

        Usage:
            for line in rag_system.stream_query("..."):
                print(line)  # m·ªói line l√† 1 JSON string (ch∆∞a wrap SSE)
        """
        initial_state = {
            "messages": conversation_history or [],
            "user_query": user_query,
            "query_type": "unknown",
            "search_results": None,
            "selected_services": [],
            "booking_info": None,
            "customer_info": None,
            "next_action": "",
            "final_response": "",
            "stream_messages": None,
            "stream_system_prompt": None,
        }

        # B∆∞·ªõc 1: Start chunk
        yield json.dumps({"type": "start", "content": None, "references": None, "status": "processing"})

        # B∆∞·ªõc 2: Ch·∫°y workflow (router + search)
        logger.info(f"üîÑ [STREAM] Running RAG workflow for: {user_query}")
        final_state = self.streaming_workflow.invoke(initial_state)

        stream_messages = final_state.get("stream_messages")
        query_type = final_state.get("query_type", "tourism")

        if not stream_messages:
            logger.error("‚ùå [STREAM] No stream_messages found in state")
            yield json.dumps({"type": "error", "content": "Xin l·ªói, c√≥ l·ªói x·∫£y ra.", "references": None, "status": "error"})
            return

        logger.info(f"‚úÖ [STREAM] Workflow done (type={query_type}), starting LLM stream...")

        # B∆∞·ªõc 3: Stream LLM realtime - m·ªói chunk l√† token th·∫≠t t·ª´ OpenAI
        chunk_count = 0
        try:
            for chunk in self._streaming_llm.stream(stream_messages):
                if chunk.content:
                    chunk_count += 1
                    yield json.dumps({"type": "chunk", "content": chunk.content, "references": None, "status": None})
        except Exception as e:
            logger.error(f"‚ùå [STREAM] LLM streaming error: {e}")
            yield json.dumps({"type": "error", "content": str(e), "references": None, "status": "error"})
            return

        logger.info(f"‚úÖ [STREAM] Done, streamed {chunk_count} chunks")

        # B∆∞·ªõc 4: End chunk
        yield json.dumps({"type": "end", "content": None, "references": None, "status": "done"})

    async def astream_query(
        self,
        user_query: str,
        conversation_history: List = None
    ) -> AsyncGenerator[str, None]:
        """
        ‚≠ê ASYNC STREAMING: Ch·∫°y workflow ƒë·ªÉ search, sau ƒë√≥ async stream LLM response.

        Yield t·ª´ng JSON string theo format:
            {"type": "start",  "content": null, "references": null, "status": "processing"}
            {"type": "chunk",  "content": "text...", "references": null, "status": null}
            {"type": "end",    "content": null, "references": null, "status": "done"}
            {"type": "error",  "content": "msg", "references": null, "status": "error"}

        D√πng cho FastAPI StreamingResponse v·ªõi asyncio.

        Usage:
            async for json_str in rag_system.astream_query("..."):
                # json_str l√† JSON string, caller wrap th√†nh SSE: f"data: {json_str}\\n\\n"
        """
        import asyncio

        initial_state = {
            "messages": conversation_history or [],
            "user_query": user_query,
            "query_type": "unknown",
            "search_results": None,
            "selected_services": [],
            "booking_info": None,
            "customer_info": None,
            "next_action": "",
            "final_response": "",
            "stream_messages": None,
            "stream_system_prompt": None,
        }

        # B∆∞·ªõc 1: Start chunk
        yield json.dumps({"type": "start", "content": None, "references": None, "status": "processing"})

        # B∆∞·ªõc 2: Ch·∫°y workflow trong thread pool (LangGraph invoke l√† sync)
        loop = asyncio.get_event_loop()
        final_state = await loop.run_in_executor(
            None,
            lambda: self.streaming_workflow.invoke(initial_state)
        )

        stream_messages = final_state.get("stream_messages")
        query_type = final_state.get("query_type", "tourism")

        if not stream_messages:
            logger.error("‚ùå [ASTREAM] No stream_messages found in state")
            yield json.dumps({"type": "error", "content": "Xin l·ªói, c√≥ l·ªói x·∫£y ra.", "references": None, "status": "error"})
            return

        logger.info(f"‚úÖ [ASTREAM] Workflow done (type={query_type}), starting async LLM stream...")

        # B∆∞·ªõc 3: Async stream LLM realtime - m·ªói chunk l√† token th·∫≠t t·ª´ OpenAI
        chunk_count = 0
        try:
            async for chunk in self._streaming_llm.astream(stream_messages):
                if chunk.content:
                    chunk_count += 1
                    yield json.dumps({"type": "chunk", "content": chunk.content, "references": None, "status": None})
        except Exception as e:
            logger.error(f"‚ùå [ASTREAM] LLM streaming error: {e}")
            yield json.dumps({"type": "error", "content": str(e), "references": None, "status": "error"})
            return

        logger.info(f"‚úÖ [ASTREAM] Done, streamed {chunk_count} chunks")

        # B∆∞·ªõc 4: End chunk
        yield json.dumps({"type": "end", "content": None, "references": None, "status": "done"})


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()

    rag_system = BaiChayRAGSystem(openai_model="gpt-4o")

    # Test non-streaming
    print("=" * 80)
    print("TEST NON-STREAMING:")
    result = rag_system.process_query("T√¨m kh√°ch s·∫°n 4 sao g·∫ßn bi·ªÉn B√£i Ch√°y")
    print(f"Query type: {result['query_type']}")
    print(f"Response: {result['response'][:300]}...")

    # Test sync streaming
    print("\n" + "=" * 80)
    print("TEST SYNC STREAMING:")
    for chunk in rag_system.stream_query("G·ª£i √Ω nh√† h√†ng h·∫£i s·∫£n ngon"):
        print(chunk, end="", flush=True)
    print()