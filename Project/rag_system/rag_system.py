"""
rag_system.py ‚Äî Giao di·ªán ch√≠nh c·ªßa h·ªá th·ªëng RAG B√£i Ch√°y v·ªõi streaming support.
UPDATED: Th√™m astream_query cho real-time streaming
FIXED: Kh√¥ng ph·ª• thu·ªôc v√†o rag_multi_agent_system

S·ª≠ d·ª•ng:
    from rag_system.rag_system import BaiChayRAGSystem

    # Non-streaming
    rag = BaiChayRAGSystem()
    result = rag.process_query("T√¨m kh√°ch s·∫°n 4 sao g·∫ßn bi·ªÉn")
    print(result["response"])

    # Streaming
    async for json_str in rag.astream_query("T√¨m kh√°ch s·∫°n 4 sao g·∫ßn bi·ªÉn"):
        print(json_str)  # JSON string cho m·ªói chunk
"""

import logging
import json
import asyncio
import os
from typing import Dict, List, Optional, AsyncGenerator

from Project.workflow.workflow import build_rag_workflow
from Project.state.state import AgentState
from Project.tools.tools import RAGTools
from Project.agents.agents import TourismAgents
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END

logger = logging.getLogger(__name__)


class BaiChayRAGSystem:
    """
    Facade cho to√†n b·ªô h·ªá th·ªëng multi-agent RAG B√£i Ch√°y v·ªõi streaming support.

    Workflow b√™n trong:
        router  ‚Üí  tourism_advisor  |  document_advisor  |  booking_agent
    """

    def __init__(
        self,
        openai_model: str = "gpt-4o",
        milvus_host: str = "localhost",
        milvus_port: str = "19530",
    ):
        logger.info("üöÄ Initializing B√£i Ch√°y RAG System...")
        logger.info(f"   Model: {openai_model}")
        logger.info("   ‚úÖ Streaming support enabled")

        self.openai_model = openai_model
        self.milvus_host = milvus_host
        self.milvus_port = milvus_port

        # Non-streaming workflow
        self.workflow = build_rag_workflow(
            openai_model=openai_model,
            milvus_host=milvus_host,
            milvus_port=milvus_port,
        )

        # Streaming workflow - s·∫Ω build khi c·∫ßn
        self._streaming_workflow = None
        self._streaming_llm = None
        self._tools = None

        logger.info("‚úÖ RAG System ready! (non-stream + stream)")

    def _build_streaming_workflow(self):
        """
        Build streaming workflow - kh√¥ng invoke LLM trong agent nodes
        """
        if self._streaming_workflow is None:
            logger.info("üîÑ Building streaming workflow...")

            # Initialize tools
            self._tools = RAGTools(
                milvus_host=self.milvus_host,
                milvus_port=self.milvus_port
            )

            # Create streaming agents (kh√¥ng invoke LLM)
            class StreamingAgents(TourismAgents):
                """
                Agents cho streaming - ch·ªâ search, kh√¥ng invoke LLM
                """

                def tourism_advisor_agent(self, state: AgentState) -> AgentState:
                    """Tourism advisor: ch·ªâ search, KH√îNG invoke LLM"""
                    logger.info("üèñÔ∏è [STREAM] Tourism Advisor: searching only...")
                    user_query = state["user_query"]

                    # Search only
                    search_results = self.tools.search_tourism_services(
                        query=user_query, top_k=5
                    )

                    try:
                        state["search_results"] = json.loads(search_results)
                    except:
                        state["search_results"] = {}

                    # Prepare messages for streaming (kh√¥ng invoke)
                    from langchain_core.messages import SystemMessage, HumanMessage

                    system_prompt = """B·∫°n l√† chuy√™n gia t∆∞ v·∫•n du l·ªãch B√£i Ch√°y - Qu·∫£ng Ninh.

NHI·ªÜM V·ª§:
D·ª±a v√†o k·∫øt qu·∫£ t√¨m ki·∫øm, t∆∞ v·∫•n cho kh√°ch h√†ng v·ªÅ c√°c d·ªãch v·ª• du l·ªãch.

FORMAT TR·∫¢ L·ªúI B·∫ÆT BU·ªòC:
V·ªõi m·ªói d·ªãch v·ª•, tr√¨nh b√†y theo c·∫•u tr√∫c sau:

---
### üè® [T√™n d·ªãch v·ª•] {rating > 0 ? '‚≠ê [rating]/5' : ''}

**üìç ƒê·ªãa ch·ªâ:** [address ho·∫∑c location]
**üí∞ Gi√°:** [price_range]
**üìù M√¥ t·∫£:** [T√≥m t·∫Øt description, kho·∫£ng 4-5 c√¢u]
**üñºÔ∏è H√¨nh ·∫£nh:** {image_url c√≥ gi√° tr·ªã ? hi·ªÉn th·ªã URL : "Ch∆∞a c√≥ h√¨nh ·∫£nh"}
**üîó Xem chi ti·∫øt:** {url c√≥ gi√° tr·ªã ? hi·ªÉn th·ªã URL : "Li√™n h·ªá ƒë·ªÉ bi·∫øt th√™m"}
**üÜî ID ƒë·ªÉ ƒë·∫∑t:** [id]
---

NGUY√äN T·∫ÆC QUAN TR·ªåNG:
1. ‚úÖ LU√îN LU√îN hi·ªÉn th·ªã image_url n·∫øu c√≥
2. ‚úÖ LU√îN LU√îN hi·ªÉn th·ªã url b√†i vi·∫øt n·∫øu c√≥
3. ‚úÖ S·∫Øp x·∫øp theo similarity_score (cao nh·∫•t tr∆∞·ªõc)
4. ‚úÖ K·∫øt th√∫c b·∫±ng c√¢u h·ªèi booking

PHONG C√ÅCH: Th√¢n thi·ªán, nhi·ªát t√¨nh, chuy√™n nghi·ªáp."""

                    llm_messages = [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=f"C√¢u h·ªèi: {user_query}\n\nK·∫øt qu·∫£ t√¨m ki·∫øm:\n{search_results}\n\nH√£y t∆∞ v·∫•n cho kh√°ch h√†ng.")
                    ]

                    state["stream_messages"] = llm_messages
                    state["stream_system_prompt"] = system_prompt
                    state["final_response"] = ""
                    state["next_action"] = "stream"

                    logger.info(f"‚úÖ [STREAM] Search done, ready for streaming")
                    return state

                def document_advisor_agent(self, state: AgentState) -> AgentState:
                    """Document advisor: ch·ªâ search, KH√îNG invoke LLM"""
                    logger.info("üìö [STREAM] Document Advisor: searching only...")
                    user_query = state["user_query"]

                    search_results = self.tools.search_documents.invoke({
                        "query": user_query, "top_k": 3
                    })

                    try:
                        state["search_results"] = json.loads(search_results)
                    except:
                        state["search_results"] = {}

                    from langchain_core.messages import SystemMessage, HumanMessage

                    system_prompt = """B·∫°n l√† chuy√™n gia t∆∞ v·∫•n quy ƒë·ªãnh du l·ªãch B√£i Ch√°y.

NHI·ªÜM V·ª§:
1. ƒê·ªçc k·ªπ n·ªôi dung t√†i li·ªáu t√¨m ƒë∆∞·ª£c
2. Tr·∫£ l·ªùi ch√≠nh x√°c d·ª±a tr√™n t√†i li·ªáu
3. Tr√≠ch d·∫´n ngu·ªìn (document_id) n·∫øu c√≥

NGUY√äN T·∫ÆC:
- Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu t√¨m ƒë∆∞·ª£c
- N·∫øu kh√¥ng t√¨m th·∫•y: "T√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin n√†y trong t√†i li·ªáu"
- Tr√¨nh b√†y r√µ r√†ng, d·ªÖ hi·ªÉu
- G·ª£i √Ω li√™n h·ªá hotline n·∫øu c·∫ßn"""

                    llm_messages = [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=f"C√¢u h·ªèi: {user_query}\n\nT√†i li·ªáu t√¨m ƒë∆∞·ª£c:\n{search_results}\n\nH√£y tr·∫£ l·ªùi c√¢u h·ªèi.")
                    ]

                    state["stream_messages"] = llm_messages
                    state["stream_system_prompt"] = system_prompt
                    state["final_response"] = ""
                    state["next_action"] = "stream"

                    return state

                def booking_agent(self, state: AgentState) -> AgentState:
                    """Booking agent: chu·∫©n b·ªã context, KH√îNG invoke LLM"""
                    logger.info("üé´ [STREAM] Booking Agent working...")
                    user_query = state["user_query"]
                    messages_history = state["messages"]

                    conversation_text = "\n".join([
                        f"{msg.__class__.__name__}: {msg.content}"
                        for msg in messages_history[-3:] if hasattr(msg, 'content')
                    ])

                    from langchain_core.messages import SystemMessage, HumanMessage

                    system_prompt = """B·∫°n l√† chuy√™n vi√™n ƒë·∫∑t tour du l·ªãch B√£i Ch√°y.

NHI·ªÜM V·ª§:
1. Thu th·∫≠p ƒë·∫ßy ƒë·ªß th√¥ng tin:
   - H·ªç t√™n kh√°ch h√†ng
   - S·ªë ƒëi·ªán tho·∫°i
   - ID d·ªãch v·ª• ƒë√£ ch·ªçn
   - Ng√†y check-in (YYYY-MM-DD)
   - Ng√†y check-out (YYYY-MM-DD)

2. N·∫øu ƒê·ª¶ th√¥ng tin: Tr·∫£ v·ªÅ JSON
3. N·∫øu THI·∫æU: H·ªèi th√™m th√¥ng tin"""

                    llm_messages = [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=f"L·ªãch s·ª≠ h·ªôi tho·∫°i:\n{conversation_text}\n\nTin nh·∫Øn m·ªõi: {user_query}\n\nPh√¢n t√≠ch v√† x·ª≠ l√Ω.")
                    ]

                    state["stream_messages"] = llm_messages
                    state["stream_system_prompt"] = system_prompt
                    state["final_response"] = ""
                    state["next_action"] = "stream"

                    return state

            # Build workflow
            agents = StreamingAgents(self._tools, openai_model=self.openai_model)

            workflow = StateGraph(AgentState)
            workflow.add_node("router", agents.router_agent)
            workflow.add_node("tourism_advisor", agents.tourism_advisor_agent)
            workflow.add_node("document_advisor", agents.document_advisor_agent)
            workflow.add_node("booking_agent", agents.booking_agent)

            workflow.set_entry_point("router")

            def route_query(state: AgentState) -> str:
                query_type = state.get("query_type", "tourism")
                if query_type == "document":
                    return "document_advisor"
                elif query_type == "booking":
                    return "booking_agent"
                else:
                    return "tourism_advisor"

            workflow.add_conditional_edges(
                "router",
                route_query,
                {
                    "tourism_advisor": "tourism_advisor",
                    "document_advisor": "document_advisor",
                    "booking_agent": "booking_agent"
                }
            )

            workflow.add_edge("tourism_advisor", END)
            workflow.add_edge("document_advisor", END)
            workflow.add_edge("booking_agent", END)

            self._streaming_workflow = workflow.compile()

            # LLM ri√™ng ƒë·ªÉ stream
            api_key = os.getenv("OPENAI_API_KEY")
            self._streaming_llm = ChatOpenAI(
                model=self.openai_model,
                temperature=0.1,
                streaming=True,
                api_key=api_key
            )

            logger.info("‚úÖ Streaming workflow ready")

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def process_query(
        self,
        user_query: str,
        conversation_history: Optional[List] = None,
    ) -> Dict:
        """
        X·ª≠ l√Ω c√¢u h·ªèi qua workflow multi-agent (non-streaming).

        Args:
            user_query: C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
            conversation_history: L·ªãch s·ª≠ h·ªôi tho·∫°i (list LangChain messages).

        Returns:
            Dict v·ªõi keys: response, query_type, messages.
        """
        initial_state = {
            "messages": conversation_history or [],
            "user_query": user_query,
            "query_type": "unknown",
            "search_results": None,
            "selected_services": [],
            "booking_info": None,
            "customer_info": None,
            "next_action": "",
            "final_response": "",
            "stream_messages": None,
            "stream_system_prompt": None,
        }

        logger.info(f"üîÑ [NON-STREAM] Processing query: {user_query}")
        final_state = self.workflow.invoke(initial_state)

        return {
            "response": final_state.get("final_response", "Xin l·ªói, t√¥i ch∆∞a hi·ªÉu c√¢u h·ªèi."),
            "query_type": final_state.get("query_type"),
            "messages": final_state.get("messages", []),
        }

    def question(
        self,
        question: str,
        history: Optional[List] = None,
    ) -> Dict:
        """Alias c·ªßa process_query ‚Äî t∆∞∆°ng th√≠ch v·ªõi endpoint /chat."""
        return self.process_query(user_query=question, conversation_history=history)

    async def astream_query(
        self,
        user_query: str,
        conversation_history: Optional[List] = None
    ) -> AsyncGenerator[str, None]:
        """
        ‚≠ê ASYNC STREAMING: Ch·∫°y workflow ƒë·ªÉ search, sau ƒë√≥ async stream LLM response.

        Yield t·ª´ng JSON string theo format:
            {"type": "start",  "content": null, "references": null, "status": "processing"}
            {"type": "chunk",  "content": "text...", "references": null, "status": null}
            {"type": "end",    "content": null, "references": null, "status": "done"}
            {"type": "error",  "content": "msg", "references": null, "status": "error"}

        Usage:
            async for json_str in rag_system.astream_query("..."):
                yield f"data: {json_str}\\n\\n"
        """
        # Lazy build streaming workflow
        self._build_streaming_workflow()

        initial_state = {
            "messages": conversation_history or [],
            "user_query": user_query,
            "query_type": "unknown",
            "search_results": None,
            "selected_services": [],
            "booking_info": None,
            "customer_info": None,
            "next_action": "",
            "final_response": "",
            "stream_messages": None,
            "stream_system_prompt": None,
        }

        # B∆∞·ªõc 1: Start chunk
        yield json.dumps({
            "type": "start",
            "content": None,
            "references": None,
            "status": "processing"
        })

        # B∆∞·ªõc 2: Ch·∫°y workflow trong thread pool (LangGraph invoke l√† sync)
        loop = asyncio.get_event_loop()
        final_state = await loop.run_in_executor(
            None,
            lambda: self._streaming_workflow.invoke(initial_state)
        )

        stream_messages = final_state.get("stream_messages")
        query_type = final_state.get("query_type", "tourism")

        if not stream_messages:
            logger.error("‚ùå [ASTREAM] No stream_messages found in state")
            yield json.dumps({
                "type": "error",
                "content": "Xin l·ªói, c√≥ l·ªói x·∫£y ra.",
                "references": None,
                "status": "error"
            })
            return

        logger.info(f"‚úÖ [ASTREAM] Workflow done (type={query_type}), starting async LLM stream...")

        # B∆∞·ªõc 3: Async stream LLM realtime
        chunk_count = 0
        try:
            async for chunk in self._streaming_llm.astream(stream_messages):
                if chunk.content:
                    chunk_count += 1
                    yield json.dumps({
                        "type": "chunk",
                        "content": chunk.content,
                        "references": None,
                        "status": None
                    })
        except Exception as e:
            logger.error(f"‚ùå [ASTREAM] LLM streaming error: {e}")
            yield json.dumps({
                "type": "error",
                "content": str(e),
                "references": None,
                "status": "error"
            })
            return

        logger.info(f"‚úÖ [ASTREAM] Done, streamed {chunk_count} chunks")

        # B∆∞·ªõc 4: End chunk
        yield json.dumps({
            "type": "end",
            "content": None,
            "references": None,
            "status": "done"
        })