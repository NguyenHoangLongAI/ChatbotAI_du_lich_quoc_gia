"""
rag_system.py ‚Äî Giao di·ªán ch√≠nh c·ªßa h·ªá th·ªëng RAG B√£i Ch√°y v·ªõi streaming support.
FIXED: Streaming workflow ƒë·ªìng b·ªô v·ªõi non-streaming workflow
"""

import logging
import json
import asyncio
import os
from typing import Dict, List, Optional, AsyncGenerator

from Project.workflow.workflow import build_rag_workflow
from Project.state.state import AgentState
from Project.tools.tools import RAGTools
from Project.agents.base_agent import BaseAgent
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langchain_core.messages import SystemMessage, HumanMessage

logger = logging.getLogger(__name__)


class BaiChayRAGSystem:
    """
    Facade cho to√†n b·ªô h·ªá th·ªëng multi-agent RAG B√£i Ch√°y v·ªõi streaming support.

    Workflow b√™n trong:
        router ‚Üí hello | human | tourism_advisor | document_advisor | booking_agent
    """

    def __init__(
        self,
        openai_model: str = "gpt-4o",
        milvus_host: str = "localhost",
        milvus_port: str = "19530",
    ):
        logger.info("üöÄ Initializing B√£i Ch√°y RAG System...")
        logger.info(f"   Model: {openai_model}")
        logger.info("   ‚úÖ Streaming support enabled")

        self.openai_model = openai_model
        self.milvus_host = milvus_host
        self.milvus_port = milvus_port

        # Non-streaming workflow
        self.workflow = build_rag_workflow(
            openai_model=openai_model,
            milvus_host=milvus_host,
            milvus_port=milvus_port,
        )

        # Streaming workflow - s·∫Ω build khi c·∫ßn
        self._streaming_workflow = None
        self._streaming_llm = None
        self._tools = None

        logger.info("‚úÖ RAG System ready! (non-stream + stream)")

    def _build_streaming_workflow(self):
        """Build streaming workflow ƒê·ªíNG B·ªò v·ªõi non-streaming workflow"""
        if self._streaming_workflow is None:
            logger.info("üîÑ Building streaming workflow...")

            # Initialize tools
            self._tools = RAGTools(
                milvus_host=self.milvus_host,
                milvus_port=self.milvus_port
            )

            # Import agents
            from Project.agents import (
                RouterAgent,
                HelloAgent,
                HumanAgent,
                TourismAdvisorAgent,
                DocumentAdvisorAgent,
                BookingAgent
            )

            # ========== STREAMING AGENTS ==========

            class StreamingHelloAgent(HelloAgent):
                """Hello agent cho streaming - kh√¥ng invoke LLM"""
                def process(self, state: AgentState) -> AgentState:
                    logger.info("üëã [STREAM] Hello Agent preparing greeting...")

                    user_query = state["user_query"].lower()
                    is_greeting = any(
                        keyword in user_query
                        for keyword in ["xin ch√†o", "ch√†o", "hello", "hi", "hey", "ch√†o b·∫°n"]
                    )

                    message_count = len(state.get("messages", []))
                    is_first_message = message_count == 0

                    if is_greeting or is_first_message:
                        llm_messages = [
                            SystemMessage(content=self.system_prompt),
                            HumanMessage(
                                content=(
                                    f"Kh√°ch h√†ng n√≥i: '{state['user_query']}'\n\n"
                                    f"S·ªë tin nh·∫Øn tr∆∞·ªõc ƒë√≥: {message_count}\n"
                                    "H√£y ch√†o h·ªèi kh√°ch h√†ng m·ªôt c√°ch th√¢n thi·ªán v√† gi·ªõi thi·ªáu d·ªãch v·ª•."
                                )
                            ),
                        ]

                        state["stream_messages"] = llm_messages
                        state["stream_system_prompt"] = self.system_prompt
                        state["final_response"] = ""
                        state["query_type"] = "hello"
                        state["next_action"] = "stream"
                    else:
                        # Kh√¥ng ph·∫£i l·ªùi ch√†o
                        logger.info("‚ÑπÔ∏è Not a greeting, will route to appropriate agent")
                        state["next_action"] = "route"

                    return state

            class StreamingHumanAgent(HumanAgent):
                """Human agent cho streaming - kh√¥ng invoke LLM, ch·ªâ extract info"""
                def process(self, state: AgentState) -> AgentState:
                    logger.info("üß† [STREAM] Human Agent analyzing context...")

                    # G·ªçi parent process ƒë·ªÉ extract customer info
                    # Nh∆∞ng kh√¥ng invoke LLM, ch·ªâ extract
                    conversation_history = self._build_conversation_context(state)
                    full_context = conversation_history + f"\n\nTin nh·∫Øn m·ªõi: {state['user_query']}"

                    # Fallback extraction (regex-based, kh√¥ng c·∫ßn LLM)
                    extracted_info = self._fallback_extraction(full_context)

                    # Merge v·ªõi customer_info hi·ªán t·∫°i
                    current_info = state.get("customer_info", {})
                    merged_info = self._merge_customer_info(current_info, extracted_info)

                    # Update state
                    state["customer_info"] = merged_info

                    logger.info(f"‚úÖ Extracted info: Name={merged_info.get('name')}, "
                               f"Phone={merged_info.get('phone')}, "
                               f"Check-in={merged_info.get('checkin')}, "
                               f"Check-out={merged_info.get('checkout')}")

                    # Determine next action
                    missing = merged_info.get("missing_fields", [])
                    if not missing or len(missing) == 0:
                        logger.info("‚úÖ All info collected, ready for booking")
                        state["next_action"] = "booking"
                    else:
                        logger.info(f"‚ÑπÔ∏è Still missing: {missing}")
                        # T·∫°o response h·ªèi th√¥ng tin c√≤n thi·∫øu
                        missing_str = ", ".join(missing)
                        state["final_response"] = (
                            f"ƒê·ªÉ ho√†n t·∫•t ƒë·∫∑t ch·ªó, t√¥i c·∫ßn th√™m th√¥ng tin sau: {missing_str}. "
                            "B·∫°n c√≥ th·ªÉ cung c·∫•p gi√∫p t√¥i kh√¥ng?"
                        )
                        state["next_action"] = "end"

                    return state

            class StreamingTourismAdvisor(TourismAdvisorAgent):
                """Tourism advisor cho streaming - kh√¥ng invoke LLM"""
                def process(self, state: AgentState) -> AgentState:
                    logger.info("üèñÔ∏è [STREAM] Tourism Advisor: searching only...")

                    # S·ª≠ d·ª•ng contextualized_query n·∫øu c√≥
                    search_query = state.get("contextualized_query", state["user_query"])

                    # Log context info n·∫øu l√† follow-up
                    context_info = state.get("context_info", {})
                    if context_info.get("is_followup"):
                        logger.info(f"üîç Using contextualized query: {search_query}")
                        logger.info(f"   Context: {context_info.get('context_summary')}")

                    search_results = self.tools.search_tourism_services(
                        query=search_query, top_k=5
                    )

                    try:
                        state["search_results"] = json.loads(search_results)
                    except:
                        state["search_results"] = {}

                    llm_messages = [
                        SystemMessage(content=self.system_prompt),
                        HumanMessage(
                            content=(
                                f"C√¢u h·ªèi g·ªëc: {state['user_query']}\n"
                                f"C√¢u h·ªèi ƒë√£ l√†m r√µ: {search_query}\n\n"
                                f"K·∫øt qu·∫£ t√¨m ki·∫øm:\n{search_results}\n\n"
                                "H√£y t∆∞ v·∫•n cho kh√°ch h√†ng."
                            )
                        )
                    ]

                    state["stream_messages"] = llm_messages
                    state["stream_system_prompt"] = self.system_prompt
                    state["final_response"] = ""
                    state["next_action"] = "stream"
                    return state

            class StreamingDocumentAdvisor(DocumentAdvisorAgent):
                """Document advisor cho streaming - kh√¥ng invoke LLM"""
                def process(self, state: AgentState) -> AgentState:
                    logger.info("üìö [STREAM] Document Advisor: searching only...")

                    # S·ª≠ d·ª•ng contextualized_query n·∫øu c√≥
                    search_query = state.get("contextualized_query", state["user_query"])

                    # Log context info
                    context_info = state.get("context_info", {})
                    if context_info.get("is_followup"):
                        logger.info(f"üîç Using contextualized query: {search_query}")
                        logger.info(f"   Context: {context_info.get('context_summary')}")

                    search_results = self.tools.search_documents.invoke({
                        "query": search_query, "top_k": 3
                    })

                    try:
                        state["search_results"] = json.loads(search_results)
                    except:
                        state["search_results"] = {}

                    llm_messages = [
                        SystemMessage(content=self.system_prompt),
                        HumanMessage(
                            content=(
                                f"C√¢u h·ªèi g·ªëc: {state['user_query']}\n"
                                f"C√¢u h·ªèi ƒë√£ l√†m r√µ: {search_query}\n\n"
                                f"T√†i li·ªáu t√¨m ƒë∆∞·ª£c:\n{search_results}\n\n"
                                "H√£y tr·∫£ l·ªùi c√¢u h·ªèi."
                            )
                        )
                    ]

                    state["stream_messages"] = llm_messages
                    state["stream_system_prompt"] = self.system_prompt
                    state["final_response"] = ""
                    state["next_action"] = "stream"
                    return state

            class StreamingBookingAgent(BookingAgent):
                """Booking agent cho streaming - kh√¥ng invoke LLM"""
                def process(self, state: AgentState) -> AgentState:
                    logger.info("üé´ [STREAM] Booking Agent working...")

                    # L·∫•y th√¥ng tin kh√°ch h√†ng t·ª´ state
                    customer_info = state.get("customer_info", {})

                    # Build context
                    conversation_text = "\n".join([
                        f"{msg.__class__.__name__}: {msg.content}"
                        for msg in state["messages"][-3:] if hasattr(msg, 'content')
                    ])

                    # Chu·∫©n b·ªã th√¥ng tin cho LLM
                    info_summary = self._format_customer_info(customer_info)

                    llm_messages = [
                        SystemMessage(content=self.system_prompt),
                        HumanMessage(
                            content=(
                                f"L·ªãch s·ª≠ h·ªôi tho·∫°i:\n{conversation_text}\n\n"
                                f"Th√¥ng tin kh√°ch h√†ng hi·ªán c√≥:\n{info_summary}\n\n"
                                f"Tin nh·∫Øn m·ªõi: {state['user_query']}\n\n"
                                "Ph√¢n t√≠ch v√† x·ª≠ l√Ω booking."
                            )
                        ),
                    ]

                    state["stream_messages"] = llm_messages
                    state["stream_system_prompt"] = self.system_prompt
                    state["final_response"] = ""
                    state["next_action"] = "stream"
                    return state

            # Initialize streaming agents
            router = RouterAgent(tools=self._tools, openai_model=self.openai_model)
            hello = StreamingHelloAgent(tools=self._tools, openai_model=self.openai_model)
            human = StreamingHumanAgent(tools=self._tools, openai_model=self.openai_model)
            tourism_advisor = StreamingTourismAdvisor(tools=self._tools, openai_model=self.openai_model)
            document_advisor = StreamingDocumentAdvisor(tools=self._tools, openai_model=self.openai_model)
            booking = StreamingBookingAgent(tools=self._tools, openai_model=self.openai_model)

            # ========== BUILD WORKFLOW (ƒê·ªíNG B·ªò V·ªöI NON-STREAMING) ==========

            workflow = StateGraph(AgentState)

            # Add nodes
            workflow.add_node("router", router.process)
            workflow.add_node("hello", hello.process)
            workflow.add_node("human", human.process)
            workflow.add_node("tourism_advisor", tourism_advisor.process)
            workflow.add_node("document_advisor", document_advisor.process)
            workflow.add_node("booking_agent", booking.process)

            # Entry point
            workflow.set_entry_point("router")

            # Routing t·ª´ router
            def route_query(state: AgentState) -> str:
                query_type = state.get("query_type", "tourism")
                if query_type == "hello":
                    return "hello"
                if query_type == "human":
                    return "human"
                if query_type == "document":
                    return "document_advisor"
                if query_type == "booking":
                    return "booking_agent"
                return "tourism_advisor"

            workflow.add_conditional_edges(
                "router",
                route_query,
                {
                    "hello": "hello",
                    "human": "human",
                    "tourism_advisor": "tourism_advisor",
                    "document_advisor": "document_advisor",
                    "booking_agent": "booking_agent"
                }
            )

            # Routing t·ª´ human agent
            def route_after_human(state: AgentState) -> str:
                next_action = state.get("next_action", "continue")
                if next_action == "booking":
                    return "booking_agent"
                else:
                    return "end"

            workflow.add_conditional_edges(
                "human",
                route_after_human,
                {
                    "booking_agent": "booking_agent",
                    "end": END
                }
            )

            # All agents end
            workflow.add_edge("hello", END)
            workflow.add_edge("tourism_advisor", END)
            workflow.add_edge("document_advisor", END)
            workflow.add_edge("booking_agent", END)

            self._streaming_workflow = workflow.compile()

            # LLM ri√™ng ƒë·ªÉ stream
            api_key = os.getenv("OPENAI_API_KEY")
            self._streaming_llm = ChatOpenAI(
                model=self.openai_model,
                temperature=0.1,
                streaming=True,
                api_key=api_key
            )

            logger.info("‚úÖ Streaming workflow ready - ƒê·ªíNG B·ªò v·ªõi non-streaming")

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def process_query(
        self,
        user_query: str,
        conversation_history: Optional[List] = None,
    ) -> Dict:
        """
        X·ª≠ l√Ω c√¢u h·ªèi qua workflow multi-agent (non-streaming).

        Args:
            user_query: C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
            conversation_history: L·ªãch s·ª≠ h·ªôi tho·∫°i (list LangChain messages).

        Returns:
            Dict v·ªõi keys: response, query_type, messages.
        """
        initial_state = {
            "messages": conversation_history or [],
            "user_query": user_query,
            "contextualized_query": user_query,
            "context_info": None,
            "query_type": "unknown",
            "search_results": None,
            "selected_services": [],
            "booking_info": None,
            "customer_info": None,
            "next_action": "",
            "final_response": "",
            "stream_messages": None,
            "stream_system_prompt": None,
        }

        logger.info(f"üîÑ [NON-STREAM] Processing query: {user_query}")
        final_state = self.workflow.invoke(initial_state)

        return {
            "response": final_state.get("final_response", "Xin l·ªói, t√¥i ch∆∞a hi·ªÉu c√¢u h·ªèi."),
            "query_type": final_state.get("query_type"),
            "messages": final_state.get("messages", []),
        }

    def question(
        self,
        question: str,
        history: Optional[List] = None,
    ) -> Dict:
        """Alias c·ªßa process_query ‚Äî t∆∞∆°ng th√≠ch v·ªõi endpoint /chat."""
        return self.process_query(user_query=question, conversation_history=history)

    async def astream_query(
        self,
        user_query: str,
        conversation_history: Optional[List] = None
    ) -> AsyncGenerator[str, None]:
        """
        ‚≠ê ASYNC STREAMING: Ch·∫°y workflow ƒê·ªíNG B·ªò v·ªõi non-streaming, sau ƒë√≥ async stream LLM response.

        Yield t·ª´ng JSON string theo format:
            {"type": "start",  "content": null, "references": null, "status": "processing"}
            {"type": "chunk",  "content": "text...", "references": null, "status": null}
            {"type": "end",    "content": null, "references": null, "status": "done"}
            {"type": "error",  "content": "msg", "references": null, "status": "error"}
        """
        # Lazy build streaming workflow
        self._build_streaming_workflow()

        initial_state = {
            "messages": conversation_history or [],
            "user_query": user_query,
            "contextualized_query": user_query,
            "context_info": None,
            "query_type": "unknown",
            "search_results": None,
            "selected_services": [],
            "booking_info": None,
            "customer_info": None,
            "next_action": "",
            "final_response": "",
            "stream_messages": None,
            "stream_system_prompt": None,
        }

        # B∆∞·ªõc 1: Start chunk
        yield json.dumps({
            "type": "start",
            "content": None,
            "references": None,
            "status": "processing"
        })

        # B∆∞·ªõc 2: Ch·∫°y workflow trong thread pool
        loop = asyncio.get_event_loop()
        final_state = await loop.run_in_executor(
            None,
            lambda: self._streaming_workflow.invoke(initial_state)
        )

        stream_messages = final_state.get("stream_messages")
        query_type = final_state.get("query_type", "unknown")
        next_action = final_state.get("next_action", "")

        # N·∫øu c√≥ final_response tr·ª±c ti·∫øp (human agent thi·∫øu info)
        if final_state.get("final_response") and not stream_messages:
            yield json.dumps({
                "type": "chunk",
                "content": final_state["final_response"],
                "references": None,
                "status": None
            })
            yield json.dumps({
                "type": "end",
                "content": None,
                "references": None,
                "status": "done"
            })
            return

        if not stream_messages:
            logger.error("‚ùå [ASTREAM] No stream_messages found in state")
            yield json.dumps({
                "type": "error",
                "content": "Xin l·ªói, c√≥ l·ªói x·∫£y ra.",
                "references": None,
                "status": "error"
            })
            return

        logger.info(f"‚úÖ [ASTREAM] Workflow done (type={query_type}), starting async LLM stream...")

        # B∆∞·ªõc 3: Async stream LLM realtime
        chunk_count = 0
        try:
            async for chunk in self._streaming_llm.astream(stream_messages):
                if chunk.content:
                    chunk_count += 1
                    yield json.dumps({
                        "type": "chunk",
                        "content": chunk.content,
                        "references": None,
                        "status": None
                    })
        except Exception as e:
            logger.error(f"‚ùå [ASTREAM] LLM streaming error: {e}")
            yield json.dumps({
                "type": "error",
                "content": str(e),
                "references": None,
                "status": "error"
            })
            return

        logger.info(f"‚úÖ [ASTREAM] Done, streamed {chunk_count} chunks")

        # B∆∞·ªõc 4: End chunk
        yield json.dumps({
            "type": "end",
            "content": None,
            "references": None,
            "status": "done"
        })