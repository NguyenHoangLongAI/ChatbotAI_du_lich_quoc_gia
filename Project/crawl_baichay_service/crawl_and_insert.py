#!/usr/bin/env python3
"""
Script t√≠ch h·ª£p: Crawl d·ªØ li·ªáu t·ª´ website v√† insert v√†o Milvus
‚≠ê OPTIMIZED VERSION - CH·ªà 1 ·∫¢NH V√Ä URL B√ÄI VI·∫æT
"""
import sys
import json
import logging
from typing import Dict

# Import crawler v√† DAO
sys.path.append('/mnt/user-data/uploads')
from crawler_baichay import BaiChayCrawler
from tourism_dao import BaiChayTourismDAO

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CrawlAndInsertPipeline:
    """Pipeline ƒë·ªÉ crawl v√† insert d·ªØ li·ªáu v√†o Milvus - OPTIMIZED"""

    def __init__(self, milvus_host: str = "localhost", milvus_port: str = "19530"):
        """
        Kh·ªüi t·∫°o pipeline
        Args:
            milvus_host: Milvus server host
            milvus_port: Milvus server port
        """
        logger.info("üöÄ Initializing OPTIMIZED Crawl & Insert Pipeline...")

        # Kh·ªüi t·∫°o crawler
        logger.info("üì° Initializing crawler...")
        self.crawler = BaiChayCrawler()

        # Kh·ªüi t·∫°o DAO
        logger.info("üíæ Connecting to Milvus...")
        self.dao = BaiChayTourismDAO(host=milvus_host, port=milvus_port)

        logger.info("‚úÖ Pipeline initialized successfully!")
        logger.info("‚≠ê Using OPTIMIZED schema: 1 image_url only")

    def crawl_category_and_insert(
            self,
            category_type: str,
            max_items: int = None,
            max_pages: int = 10,
            batch_size: int = 10
    ) -> Dict:
        """
        Crawl m·ªôt category v√† insert v√†o Milvus

        Args:
            category_type: Lo·∫°i category (diem-den, luu-tru, tour, etc.)
            max_items: S·ªë items t·ªëi ƒëa ƒë·ªÉ crawl
            max_pages: S·ªë trang t·ªëi ƒëa ƒë·ªÉ crawl
            batch_size: S·ªë items trong m·ªói batch insert

        Returns:
            Dict v·ªõi th·ªëng k√™: crawled_count, inserted_count, failed_count
        """
        logger.info(f"\n{'=' * 80}")
        logger.info(f"üéØ Processing category: {category_type}")
        logger.info(f"{'=' * 80}")

        # Crawl data
        logger.info(f"üì° Crawling {category_type}...")
        crawled_data = self.crawler.crawl_category(
            category_type=category_type,
            max_items=max_items,
            max_pages=max_pages
        )

        if not crawled_data:
            logger.warning(f"‚ö†Ô∏è No data crawled from {category_type}")
            return {
                "category": category_type,
                "crawled_count": 0,
                "inserted_count": 0,
                "failed_count": 0
            }

        logger.info(f"‚úÖ Crawled {len(crawled_data)} items")

        # Ki·ªÉm tra d·ªØ li·ªáu
        with_image = sum(1 for item in crawled_data if item.get("image_url"))
        without_image = len(crawled_data) - with_image
        logger.info(f"üì∏ Items with image: {with_image}")
        logger.info(f"‚ö†Ô∏è Items without image: {without_image}")

        # Insert v√†o Milvus theo batch
        logger.info(f"üíæ Inserting into Milvus (batch size: {batch_size})...")
        inserted_count = 0
        failed_count = 0

        # T·∫°o ID duy nh·∫•t cho m·ªói item
        category_id_offset = self._get_category_id_offset(category_type)

        for i in range(0, len(crawled_data), batch_size):
            batch = crawled_data[i:i + batch_size]

            # Assign unique IDs
            for idx, item in enumerate(batch):
                item["id"] = category_id_offset + i + idx + 1

            try:
                self.dao.insert_data(batch)
                inserted_count += len(batch)
                logger.info(f"  ‚úÖ Inserted batch {i // batch_size + 1}: {len(batch)} items")
            except Exception as e:
                failed_count += len(batch)
                logger.error(f"  ‚ùå Failed to insert batch {i // batch_size + 1}: {e}")

        stats = {
            "category": category_type,
            "crawled_count": len(crawled_data),
            "inserted_count": inserted_count,
            "failed_count": failed_count,
            "items_with_image": with_image,
            "items_without_image": without_image
        }

        logger.info(f"\nüìä Category '{category_type}' Summary:")
        logger.info(f"  Crawled:  {stats['crawled_count']}")
        logger.info(f"  Inserted: {stats['inserted_count']}")
        logger.info(f"  Failed:   {stats['failed_count']}")
        logger.info(f"  üì∏ With image: {stats['items_with_image']}")
        logger.info(f"  ‚ö†Ô∏è Without image: {stats['items_without_image']}")

        return stats

    def crawl_all_and_insert(
            self,
            max_items_per_category: int = None,
            max_pages_per_category: int = 10,
            batch_size: int = 10
    ) -> Dict[str, Dict]:
        """
        Crawl t·∫•t c·∫£ categories v√† insert v√†o Milvus

        Args:
            max_items_per_category: S·ªë items t·ªëi ƒëa m·ªói category
            max_pages_per_category: S·ªë trang t·ªëi ƒëa m·ªói category
            batch_size: S·ªë items trong m·ªói batch insert

        Returns:
            Dict v·ªõi th·ªëng k√™ cho t·ª´ng category
        """
        logger.info(f"\n{'=' * 80}")
        logger.info("üåç CRAWLING AND INSERTING ALL CATEGORIES (OPTIMIZED)")
        logger.info(f"{'=' * 80}")

        all_stats = {}

        for category_type in self.crawler.CATEGORY_URLS.keys():
            try:
                stats = self.crawl_category_and_insert(
                    category_type=category_type,
                    max_items=max_items_per_category,
                    max_pages=max_pages_per_category,
                    batch_size=batch_size
                )
                all_stats[category_type] = stats
            except Exception as e:
                logger.error(f"‚ùå Error processing {category_type}: {e}")
                all_stats[category_type] = {
                    "category": category_type,
                    "crawled_count": 0,
                    "inserted_count": 0,
                    "failed_count": 0,
                    "error": str(e)
                }

        # Overall summary
        logger.info(f"\n{'=' * 80}")
        logger.info("üìä OVERALL SUMMARY")
        logger.info(f"{'=' * 80}")

        total_crawled = sum(s['crawled_count'] for s in all_stats.values())
        total_inserted = sum(s['inserted_count'] for s in all_stats.values())
        total_failed = sum(s['failed_count'] for s in all_stats.values())
        total_with_image = sum(s.get('items_with_image', 0) for s in all_stats.values())
        total_without_image = sum(s.get('items_without_image', 0) for s in all_stats.values())

        logger.info(f"Total Crawled:  {total_crawled}")
        logger.info(f"Total Inserted: {total_inserted}")
        logger.info(f"Total Failed:   {total_failed}")
        logger.info(f"üì∏ With image:  {total_with_image} ({total_with_image/total_crawled*100:.1f}%)")
        logger.info(f"‚ö†Ô∏è Without image: {total_without_image} ({total_without_image/total_crawled*100:.1f}%)")

        # Database stats
        db_stats = self.dao.get_statistics()
        logger.info(f"\nüíæ Database Statistics:")
        logger.info(f"  Database:   {db_stats['Project']}")
        logger.info(f"  Collection: {db_stats['collection']['name']}")
        logger.info(f"  Total Items: {db_stats['collection']['total_count']}")
        logger.info(f"  Schema: {db_stats['collection']['schema_version']}")

        return all_stats

    def _get_category_id_offset(self, category_type: str) -> int:
        """
        L·∫•y offset ID cho m·ªói category ƒë·ªÉ tr√°nh tr√πng ID
        M·ªói category c√≥ 10000 IDs
        """
        category_offsets = {
            "diem-den": 0,
            "luu-tru": 10000,
            "tour": 20000,
            "nha-hang": 30000,
            "am-thuc": 40000,
            "du-thuyen": 50000
        }
        return category_offsets.get(category_type, 60000)

    def export_stats_to_json(self, stats: Dict, filepath: str = "insert_stats_optimized.json"):
        """L∆∞u th·ªëng k√™ v√†o JSON file"""
        logger.info(f"üíæ Saving statistics to {filepath}...")
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        logger.info(f"‚úÖ Statistics saved!")


def main():
    """Main function"""
    print("=" * 80)
    print("üöÄ B√ÉI CH√ÅY TOURISM DATA PIPELINE - OPTIMIZED VERSION")
    print("‚≠ê CH·ªà L·∫§Y 1 ·∫¢NH THUMBNAIL V√Ä URL B√ÄI VI·∫æT")
    print("=" * 80)

    # C·∫•u h√¨nh
    MILVUS_HOST = "localhost"
    MILVUS_PORT = "19530"

    # T√πy ch·ªçn crawl
    MAX_ITEMS_PER_CATEGORY = None  # None = crawl t·∫•t c·∫£
    MAX_PAGES_PER_CATEGORY = 20  # S·ªë trang t·ªëi ƒëa m·ªói category
    BATCH_SIZE = 10  # S·ªë items insert m·ªói l·∫ßn

    try:
        # Kh·ªüi t·∫°o pipeline
        pipeline = CrawlAndInsertPipeline(
            milvus_host=MILVUS_HOST,
            milvus_port=MILVUS_PORT
        )

        # L·ª±a ch·ªçn: Crawl m·ªôt category hay t·∫•t c·∫£?
        print("\nüìã Options:")
        print("  1. Crawl and insert ONE category (for testing)")
        print("  2. Crawl and insert ALL categories")

        choice = input("\nYour choice (1 or 2): ").strip()

        if choice == "1":
            # Crawl m·ªôt category
            print("\nüìã Available categories:")
            categories = list(pipeline.crawler.CATEGORY_URLS.keys())
            for idx, cat in enumerate(categories, 1):
                print(f"  {idx}. {cat}")

            cat_choice = input(f"\nSelect category (1-{len(categories)}): ").strip()
            try:
                cat_idx = int(cat_choice) - 1
                category = categories[cat_idx]

                max_items_input = input(f"\nMax items to crawl (press Enter for all): ").strip()
                max_items = int(max_items_input) if max_items_input else None

                stats = pipeline.crawl_category_and_insert(
                    category_type=category,
                    max_items=max_items,
                    max_pages=MAX_PAGES_PER_CATEGORY,
                    batch_size=BATCH_SIZE
                )

                pipeline.export_stats_to_json({"single_category": stats})

            except (ValueError, IndexError):
                print("‚ùå Invalid choice!")
                return

        elif choice == "2":
            # Crawl t·∫•t c·∫£ categories
            all_stats = pipeline.crawl_all_and_insert(
                max_items_per_category=MAX_ITEMS_PER_CATEGORY,
                max_pages_per_category=MAX_PAGES_PER_CATEGORY,
                batch_size=BATCH_SIZE
            )

            pipeline.export_stats_to_json(all_stats)

        else:
            print("‚ùå Invalid choice!")
            return

        print("\n" + "=" * 80)
        print("‚úÖ PIPELINE COMPLETED SUCCESSFULLY!")
        print("=" * 80)

    except Exception as e:
        logger.error(f"\n‚ùå Pipeline Error: {e}")
        import traceback
        traceback.print_exc()
        print("=" * 80)


if __name__ == "__main__":
    main()