"""
FastAPI Server for B√£i Ch√°y RAG Multi-Agent System
Port: 8503
Using OpenAI GPT-4o
UPDATED: H·ªó tr·ª£ streaming LLM realtime

Streaming usage:
  POST /chat           body: {"question": "...", "stream": false}  ‚Üí JSON response (c≈©)
  POST /chat           body: {"question": "...", "stream": true}   ‚Üí SSE stream (m·ªõi)
  GET  /chat/stream?question=...                                   ‚Üí SSE stream (m·ªõi, GET)
"""

from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Union
import uvicorn
import logging
from datetime import datetime
import os
import json
from dotenv import load_dotenv
import asyncio
from rag_multi_agent_system import BaiChayRAGSystem

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="B√£i Ch√°y Tourism RAG API (OpenAI GPT-4o)",
    version="3.1.0",
    description="""
Multi-Agent RAG System using OpenAI GPT-4o with real-time streaming support.

## Streaming

Th√™m `"stream": true` v√†o body ƒë·ªÉ nh·∫≠n response d·∫°ng SSE stream:

```
POST /chat
{"question": "T√¨m kh√°ch s·∫°n g·∫ßn bi·ªÉn", "stream": true}
```

Response l√† Server-Sent Events stream:
- D√≤ng ƒë·∫ßu: `[META]{...}` ch·ª©a query_type
- C√°c d√≤ng ti·∫øp theo: text chunks t·ª´ LLM (realtime)
"""
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# CONFIGURATION
# ============================================================================

OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    logger.error("‚ùå OPENAI_API_KEY not found in environment variables!")
    raise ValueError("OPENAI_API_KEY is required")

# ============================================================================
# REQUEST/RESPONSE MODELS
# ============================================================================

class ChatMessage(BaseModel):
    role: str = Field(..., description="user or assistant")
    content: str = Field(..., description="Message content")
    timestamp: Optional[str] = Field(default=None)



class ChatRequest(BaseModel):
    question: str = Field(..., min_length=1)
    history: Optional[List[ChatMessage]] = Field(default=None)
    session_id: Optional[str] = None
    stream: Optional[bool] = Field(
        default=True,
        description="M·∫∑c ƒë·ªãnh stream SSE. Set false n·∫øu mu·ªën JSON response."
    )

class QueryRequest(BaseModel):
    query: str = Field(..., description="User's question", min_length=1)
    conversation_history: Optional[List[ChatMessage]] = Field(default=None)
    session_id: Optional[str] = Field(default=None)


class QueryResponse(BaseModel):
    response: str = Field(..., description="Assistant's response")
    query_type: str = Field(..., description="tourism | document | booking")
    timestamp: str = Field(..., description="Response timestamp")
    session_id: Optional[str] = Field(default=None)
    metadata: Optional[Dict] = Field(default=None)


# ============================================================================
# GLOBAL RAG SYSTEM
# ============================================================================

rag_system: Optional[BaiChayRAGSystem] = None


@app.on_event("startup")
async def startup_event():
    global rag_system
    try:
        logger.info("üöÄ Starting B√£i Ch√°y RAG API with OpenAI GPT-4o...")
        logger.info(f"   Model: {OPENAI_MODEL}")

        rag_system = BaiChayRAGSystem(openai_model=OPENAI_MODEL)

        logger.info("‚úÖ RAG System initialized (non-stream + stream ready)")
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize RAG system: {e}")
        raise


# ============================================================================
# HELPER: Build history t·ª´ ChatMessage list
# ============================================================================

def build_langchain_history(history: Optional[List[ChatMessage]]):
    """Convert ChatMessage list th√†nh LangChain messages"""
    from langchain_core.messages import HumanMessage, AIMessage

    lc_history = []
    if history:
        for msg in history:
            if msg.role == "user":
                lc_history.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                lc_history.append(AIMessage(content=msg.content))
    return lc_history


# ============================================================================
# HELPER: SSE Generator
# ============================================================================

async def sse_generator(user_question: str, history, session_id: Optional[str] = None):
    """
    Async generator cho SSE streaming response.

    Format m·ªói SSE event (data: ...):
        {"type": "start",  "content": null, "references": null, "status": "processing"}
        {"type": "chunk",  "content": "text token", "references": null, "status": null}
        {"type": "end",    "content": null, "references": null, "status": "done"}
        {"type": "error",  "content": "msg", "references": null, "status": "error"}

    Client: ƒë·ªçc ƒë·∫øn type=="end" ho·∫∑c type=="error" th√¨ d·ª´ng.
    """
    if not rag_system:
        error_json = json.dumps({"type": "error", "content": "RAG system not initialized", "references": None, "status": "error"})
        yield f"data: {error_json}\n\n"
        return

    try:
        async for json_str in rag_system.astream_query(
            user_query=user_question,
            conversation_history=history
        ):
            # json_str l√† JSON string t·ª´ astream_query, wrap th√†nh SSE event
            yield f"data: {json_str}\n\n"
            await asyncio.sleep(0.001)  # yield control cho event loop

    except Exception as e:
        logger.error(f"‚ùå SSE streaming error: {e}", exc_info=True)
        error_json = json.dumps({"type": "error", "content": str(e), "references": None, "status": "error"})
        yield f"data: {error_json}\n\n"


# ============================================================================
# ENDPOINTS
# ============================================================================

@app.get("/")
async def root():
    return {
        "service": "B√£i Ch√°y Tourism RAG API",
        "version": "3.1.0",
        "status": "running",
        "port": 8503,
        "llm": {
            "provider": "OpenAI",
            "model": OPENAI_MODEL,
            "streaming": "supported"
        },
        "streaming_usage": {
            "description": "Th√™m 'stream: true' v√†o POST /chat ƒë·ªÉ nh·∫≠n SSE stream",
            "example_body": {"question": "T√¨m kh√°ch s·∫°n g·∫ßn bi·ªÉn", "stream": True},
            "sse_format": {
                "meta": "data: [META]{\"query_type\": \"tourism\"}",
                "chunk": "data: text chunk from LLM",
                "done": "data: [DONE]"
            }
        },
        "endpoints": {
            "chat": "POST /chat (stream=false ‚Üí JSON, stream=true ‚Üí SSE)",
            "chat_stream_get": "GET /chat/stream?question=... (SSE)",
            "health": "GET /api/v1/health",
            "stats": "GET /api/v1/stats",
            "examples": "GET /api/v1/examples"
        }
    }


@app.post("/chat")
async def chat(request: ChatRequest):
    """
    Chat endpoint h·ªó tr·ª£ c·∫£ streaming v√† non-streaming.

    - `stream: false` (m·∫∑c ƒë·ªãnh): Tr·∫£ v·ªÅ JSON QueryResponse nh∆∞ c≈©.
    - `stream: true`: Tr·∫£ v·ªÅ SSE stream. Response l√† text/event-stream.
      Client ƒë·ªçc t·ª´ng `data:` event cho ƒë·∫øn khi g·∫∑p `data: [DONE]`.
    """
    if not rag_system:
        raise HTTPException(status_code=503, detail="RAG system not initialized")

    history = build_langchain_history(request.history)

    # ==================== STREAMING PATH ====================
    if request.stream:
        logger.info(f"üì® [STREAM] Question: {request.question}")

        return StreamingResponse(
            sse_generator(
                user_question=request.question,
                history=history,
                session_id=request.session_id
            ),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no",        # T·∫Øt nginx buffering
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
            }
        )

    # ==================== NON-STREAMING PATH (c≈©) ====================
    try:
        logger.info(f"üì® [NON-STREAM] Question: {request.question}")

        result = rag_system.question(
            question=request.question,
            history=history
        )

        return QueryResponse(
            response=result["response"],
            query_type=result.get("query_type", "unknown"),
            timestamp=datetime.now().isoformat(),
            session_id=request.session_id,
            metadata={
                "model": OPENAI_MODEL,
                "stream": False,
                "message_count": len(result.get("messages", []))
            }
        )

    except Exception as e:
        logger.error(f"‚ùå Chat error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/chat/stream")
async def chat_stream_get(
    question: str = Query(..., min_length=1, description="C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng"),
    session_id: Optional[str] = Query(default=None)
):
    """
    GET endpoint cho SSE streaming (ti·ªán cho browser/EventSource).

    V√≠ d·ª• d√πng v·ªõi JavaScript EventSource:
    ```javascript
    const evtSource = new EventSource('/chat/stream?question=T√¨m+kh√°ch+s·∫°n');
    evtSource.onmessage = (e) => {
        if (e.data === '[DONE]') { evtSource.close(); return; }
        if (e.data.startsWith('[META]')) { /* parse metadata */ return; }
        if (e.data.startsWith('[ERROR]')) { /* handle error */ return; }
        responseDiv.textContent += e.data.replace(/\\\\n/g, '\\n');
    };
    ```
    """
    if not rag_system:
        raise HTTPException(status_code=503, detail="RAG system not initialized")

    logger.info(f"üì® [STREAM GET] Question: {question}")

    return StreamingResponse(
        sse_generator(
            user_question=question,
            history=[],
            session_id=session_id
        ),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
        }
    )


@app.get("/api/v1/health")
async def health_check():
    try:
        rag_status = rag_system is not None
        openai_status = bool(OPENAI_API_KEY)

        milvus_status = False
        try:
            if rag_system and rag_system.workflow:
                milvus_status = True
        except:
            pass

        return {
            "status": "healthy" if (rag_status and openai_status and milvus_status) else "degraded",
            "service": "rag-multi-agent-api",
            "version": "3.1.0",
            "port": 8503,
            "llm": {
                "provider": "OpenAI",
                "model": OPENAI_MODEL,
                "api_key_configured": openai_status,
                "streaming": "enabled"
            },
            "components": {
                "rag_system": "ready" if rag_status else "not_ready",
                "openai": "configured" if openai_status else "not_configured",
                "milvus": "connected" if milvus_status else "disconnected",
                "agents": {
                    "router": "active",
                    "tourism_advisor": "active",
                    "document_advisor": "active",
                    "booking_agent": "active"
                }
            },
            "databases": {
                "tourism_data": "bai_chay_data",
                "documents": "document_tour",
                "customers": "customers"
            }
        }

    except Exception as e:
        return {
            "status": "unhealthy",
            "service": "rag-multi-agent-api",
            "error": str(e)
        }


@app.get("/api/v1/stats")
async def get_stats():
    try:
        if not rag_system:
            raise HTTPException(status_code=503, detail="RAG system not initialized")

        from Project.crawl_baichay_service.tourism_dao import BaiChayTourismDAO
        from Project.document_db.tourism_document_dao import TourismDocumentDAO
        from Project.baichay_db.customer_dao import CustomerDAO

        tourism_dao = BaiChayTourismDAO()
        customer_dao = CustomerDAO()

        tourism_stats = tourism_dao.get_statistics()
        customer_stats = customer_dao.get_statistics()

        return {
            "status": "success",
            "llm_provider": "OpenAI",
            "model": OPENAI_MODEL,
            "streaming_support": True,
            "statistics": {
                "tourism_services": {
                    "total_count": tourism_stats["collection"]["total_count"],
                    "collection": tourism_stats["collection"]["name"]
                },
                "documents": {
                    "collection": "document_tour",
                    "status": "active"
                },
                "customers": {
                    "total_count": customer_stats["total_customers"],
                    "collection": customer_stats["collection_name"]
                }
            },
            "workflow": {
                "nodes": [
                    "router",
                    "tourism_advisor",
                    "document_advisor",
                    "booking_agent"
                ],
                "tools": [
                    "search_tourism_services",
                    "search_documents",
                    "get_service_by_id",
                    "create_customer_booking"
                ]
            }
        }

    except Exception as e:
        logger.error(f"‚ùå Stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/examples")
async def get_examples():
    return {
        "non_streaming_example": {
            "method": "POST",
            "url": "/chat",
            "body": {
                "question": "T√¨m kh√°ch s·∫°n 4 sao g·∫ßn bi·ªÉn B√£i Ch√°y",
                "stream": False
            },
            "response_type": "application/json"
        },
        "streaming_example": {
            "method": "POST",
            "url": "/chat",
            "body": {
                "question": "T√¨m kh√°ch s·∫°n 4 sao g·∫ßn bi·ªÉn B√£i Ch√°y",
                "stream": True
            },
            "response_type": "text/event-stream",
            "sse_events": [
                "data: {\"type\": \"start\", \"content\": null, \"references\": null, \"status\": \"processing\"}",
                "data: {\"type\": \"chunk\", \"content\": \"**Kh√°ch s·∫°n M∆∞·ªùng Thanh**\", \"references\": null, \"status\": null}",
                "data: {\"type\": \"chunk\", \"content\": \" l√† m·ªôt trong nh·ªØng...\", \"references\": null, \"status\": null}",
                "data: {\"type\": \"end\", \"content\": null, \"references\": null, \"status\": \"done\"}"
            ]
        },
        "streaming_get_example": {
            "method": "GET",
            "url": "/chat/stream?question=T%C3%ACm+kh%C3%A1ch+s%E1%BA%A1n+g%E1%BA%A7n+bi%E1%BB%83n",
            "response_type": "text/event-stream"
        },
        "javascript_client_example": """
// JavaScript SSE client example
async function streamChat(question) {
    const response = await fetch('/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ question, stream: true })
    });

    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let buffer = '';

    while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\\n');
        buffer = lines.pop(); // gi·ªØ l·∫°i d√≤ng ch∆∞a ho√†n ch·ªânh

        for (const line of lines) {
            if (!line.startsWith('data: ')) continue;
            const data = line.slice(6); // b·ªè 'data: '

            const parsed = JSON.parse(data);
            if (parsed.type === 'start') { /* show loading */ continue; }
            if (parsed.type === 'end')   { evtSource && evtSource.close(); return; }
            if (parsed.type === 'error') { console.error(parsed.content); return; }
            if (parsed.type === 'chunk') {
                document.getElementById('response').textContent += parsed.content;
            }
        }
    }
}
""",
        "tourism_queries": [
            "T√¨m kh√°ch s·∫°n 4 sao g·∫ßn bi·ªÉn B√£i Ch√°y",
            "G·ª£i √Ω tour du l·ªãch H·∫° Long 2 ng√†y 1 ƒë√™m",
            "Nh√† h√†ng h·∫£i s·∫£n ngon ·ªü B√£i Ch√°y"
        ],
        "document_queries": [
            "Quy ƒë·ªãnh h·ªßy tour nh∆∞ th·∫ø n√†o?",
            "Ch√≠nh s√°ch ho√†n ti·ªÅn khi h·ªßy ƒë·∫∑t ph√≤ng"
        ],
        "booking_queries": [
            "T√¥i mu·ªën ƒë·∫∑t kh√°ch s·∫°n M∆∞·ªùng Thanh, t√™n Nguy·ªÖn VƒÉn A, SƒêT 0901234567, t·ª´ 15/03 ƒë·∫øn 17/03"
        ]
    }


if __name__ == "__main__":
    logger.info("=" * 70)
    logger.info("Starting B√£i Ch√°y Tourism RAG API v3.1.0")
    logger.info(f"LLM: OpenAI {OPENAI_MODEL}")
    logger.info("Streaming: ENABLED")
    logger.info("=" * 70)

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8503,
        log_level="info"
    )